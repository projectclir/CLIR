{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXW24tIuTgHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVcFTv2JOyBy"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKCNi8rwN8yf",
        "outputId": "63f79fbe-c620-41d1-946a-15cd39e51065"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import math\n",
        "\n",
        "def tokenize(line, tokenizer=word_tokenize):\n",
        "    utf_line = line.lower()\n",
        "    return [token for token in tokenizer(utf_line)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwK8O4NQcTdk"
      },
      "source": [
        "**Example for tokenization**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR7ezD4NO88s",
        "outputId": "4bcdae7f-06d0-413e-b498-9cbea5ee5622"
      },
      "outputs": [],
      "source": [
        "tokenize(\"Nichts ist, wie es scheint.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEQtLgdJcx4n"
      },
      "source": [
        "**Naming the path of the data files as variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GGpNF2tTdc5"
      },
      "outputs": [],
      "source": [
        "DEVELOPMENT_DOCS = '/content/drive/MyDrive/data/clir/devel.docs' \n",
        "\n",
        "DEVELOPMENT_QUERIES = '/content/drive/MyDrive/data/clir/devel.queries' \n",
        "\n",
        "DEVELOPMENT_QREL = '/content/drive/MyDrive/data/clir/devel.qrel' \n",
        "\n",
        "BITEXT_ENG = '/content/drive/MyDrive/data/clir/bitext.en' \n",
        "\n",
        "BITEXT_DE = '/content/drive/MyDrive/data/clir/bitext.de' \n",
        "\n",
        "NEWSTEST_ENG = '/content/drive/MyDrive/data/clir/newstest.en' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGlLPO2bgIN9"
      },
      "source": [
        "**Loading the devel.docs file, extracting and tokenizing the terms, and storing them in a python dictionary with the document ids as keys.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1ukayfnZ-51"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
        "stemmer = nltk.stem.PorterStemmer() \n",
        "\n",
        "def extract_and_tokenize_terms(doc):\n",
        "    terms = []\n",
        "    for token in tokenize(doc):\n",
        "        if token not in stopwords: \n",
        "            if not re.search(r'\\d',token) and not re.search(r'[^A-Za-z-]',token): #Removing numbers and punctuations \n",
        "                terms.append(stemmer.stem(token.lower()))\n",
        "    return terms\n",
        "\n",
        "documents = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnTmkRdVwLeA",
        "outputId": "f5801fcd-1e3e-42d6-adc1-4be9cbcf547a"
      },
      "outputs": [],
      "source": [
        "f = open(DEVELOPMENT_DOCS)\n",
        "\n",
        "for line in f:\n",
        "    print(line)\n",
        "    doc = line.split(\"\\t\")\n",
        "    print(doc)\n",
        "    print(doc[1])\n",
        "    print(doc[0])\n",
        "    break\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXJKXjEeYicv"
      },
      "outputs": [],
      "source": [
        "f = open(DEVELOPMENT_DOCS)\n",
        "\n",
        "for line in f:\n",
        "    doc = line.split(\"\\t\")\n",
        "    terms = extract_and_tokenize_terms(doc[1])\n",
        "    documents[doc[0]] = terms\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPd7Uvuobj5L",
        "outputId": "cf5449e0-624b-4270-ea88-e752ebce50f0"
      },
      "outputs": [],
      "source": [
        "documents['308'][:65] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHeZaIoN55h_"
      },
      "source": [
        "**Building an inverted index for the documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSsg4e295anK"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "    \n",
        "inverted_index = defaultdict(set)\n",
        "\n",
        "for docid, terms in documents.items():\n",
        "    for term in terms:\n",
        "        inverted_index[term].add(docid) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w31mwYbd6BO-",
        "outputId": "4191d70d-a2c7-424a-c151-05e82b813795"
      },
      "outputs": [],
      "source": [
        "sorted(inverted_index['state'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3uOwZRpH3oF"
      },
      "source": [
        "**Building a TF-IDF representation using BM25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UoAKvfwCpuL"
      },
      "outputs": [],
      "source": [
        "NO_DOCS = len(documents) \n",
        "\n",
        "AVG_LEN_DOC = sum([len(doc) for doc in documents.values()])/len(documents) \n",
        "\n",
        "def tf_idf_score(k1,b,term,docid):  \n",
        "    \n",
        "    ft = len(inverted_index[term]) \n",
        "    term = stemmer.stem(term.lower())\n",
        "    fdt =  documents[docid].count(term)\n",
        "    \n",
        "    idf_comp = math.log((NO_DOCS - ft + 0.5)/(ft+0.5))\n",
        "    \n",
        "    tf_comp = ((k1 + 1)*fdt)/(k1*((1-b) + b*(len(documents[docid])/AVG_LEN_DOC))+fdt)\n",
        "    \n",
        "    return idf_comp * tf_comp\n",
        "\n",
        "def create_tf_idf(k1,b):\n",
        "    tf_idf = defaultdict(dict)\n",
        "    for term in set(inverted_index.keys()):\n",
        "        for docid in inverted_index[term]:\n",
        "            tf_idf[term][docid] = tf_idf_score(k1,b,term,docid)\n",
        "    return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztWRSQiNHRIu"
      },
      "outputs": [],
      "source": [
        "tf_idf = create_tf_idf(1.6,0.875)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1be1LUyQFsU"
      },
      "outputs": [],
      "source": [
        "def get_qtf_comp(k3,term,fqt):\n",
        "    return ((k3+1)*fqt[term])/(k3 + fqt[term])\n",
        "\n",
        "\n",
        "#Function to retrieve documents || Returns a set of documents and their relevance scores. \n",
        "def retr_docs(query,result_count):\n",
        "    q_terms = [stemmer.stem(term.lower()) for term in query.split() if term not in stopwords] \n",
        "    fqt = {}\n",
        "    for term in q_terms:\n",
        "        fqt[term] = fqt.get(term,0) + 1\n",
        "    \n",
        "    scores = {}\n",
        "    \n",
        "    for word in fqt.keys():\n",
        "        for document in inverted_index[word]:\n",
        "            scores[document] = scores.get(document,0) + (tf_idf[word][document]*get_qtf_comp(1.5,word,fqt)) \n",
        "    \n",
        "    return sorted(scores.items(),key = lambda x : x[1] , reverse=True)[:result_count] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woL9RHlGQNsW",
        "outputId": "f03c6360-d03b-45e5-961c-2d3bcfe3a852"
      },
      "outputs": [],
      "source": [
        "retr_docs(\"which is the tallest building in the world?\",5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiQ07QVtQeLW",
        "outputId": "6ae39ec7-4a09-4b4c-a442-e56294fe8c67"
      },
      "outputs": [],
      "source": [
        "documents['34080'][:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3NySdS3QyXk",
        "outputId": "1a42b114-a32c-4816-de9d-c82749ea56fd"
      },
      "outputs": [],
      "source": [
        "f = open(DEVELOPMENT_DOCS)\n",
        "\n",
        "for line in f:\n",
        "    doc = line.split(\"\\t\")\n",
        "    if doc[0]=='34080':\n",
        "      print(doc[1])\n",
        "      break\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AbTcNvcLQ5z"
      },
      "outputs": [],
      "source": [
        "#Calculating the unigram, bigram and trigram counts. \n",
        "\n",
        "f = open(BITEXT_ENG)\n",
        "\n",
        "train_sentences = []\n",
        "\n",
        "for line in f:\n",
        "    train_sentences.append(tokenize(line))\n",
        "\n",
        "f.close()    \n",
        "\n",
        "#Function to mark the first occurence of words as unknown, for training.\n",
        "def check_for_unk_train(word,unigram_counts):\n",
        "    if word in unigram_counts:\n",
        "        return word\n",
        "    else:\n",
        "        unigram_counts[word] = 0\n",
        "        return \"UNK\"\n",
        "\n",
        "#Function to convert sentences for training the language model.    \n",
        "def convert_sentence_train(sentence,unigram_counts):\n",
        "    #<s1> and <s2> are sentinel tokens added to the start and end\n",
        "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s2>\"]+ [\"</s1>\"]\n",
        "\n",
        "#Function to obtain unigram, bigram and trigram counts.\n",
        "def get_counts(sentences):\n",
        "    trigram_counts = defaultdict(lambda: defaultdict(dict))\n",
        "    bigram_counts = defaultdict(dict)\n",
        "    unigram_counts = {}\n",
        "    for sentence in sentences:\n",
        "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
        "        for i in range(len(sentence) - 2):\n",
        "            trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]] = trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0) + 1\n",
        "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
        "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
        "    unigram_counts[\"</s1>\"] = unigram_counts[\"<s1>\"]\n",
        "    unigram_counts[\"</s2>\"] = unigram_counts[\"<s2>\"]\n",
        "    bigram_counts[\"</s2>\"][\"</s1>\"] = bigram_counts[\"<s1>\"][\"<s2>\"]\n",
        "    return unigram_counts, bigram_counts, trigram_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-GWkwo3pHmC"
      },
      "outputs": [],
      "source": [
        "unigram_counts, bigram_counts,trigram_counts = get_counts(train_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0wc-D2IpsV8"
      },
      "outputs": [],
      "source": [
        "#Constructing unigram model with 'add-k' smoothing\n",
        "token_count = sum(unigram_counts.values())\n",
        "\n",
        "#Function to convert unknown words for testing. \n",
        "def check_for_unk_test(word,unigram_counts):\n",
        "    if word in unigram_counts and unigram_counts[word] > 0:\n",
        "        return word\n",
        "    else:\n",
        "        return \"UNK\"\n",
        "\n",
        "\n",
        "def convert_sentence_test(sentence,unigram_counts):\n",
        "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s2>\"]  + [\"</s1>\"]\n",
        "\n",
        "#Returns the log probability of a unigram, with add-k smoothing.\n",
        "def get_log_prob_addk(word,unigram_counts,k):\n",
        "    return math.log((unigram_counts[word] + k)/ \\\n",
        "                    (token_count + k*len(unigram_counts)))\n",
        "\n",
        "#Returns the log probability of a sentence.\n",
        "def get_sent_log_prob_addk(sentence, unigram_counts,k):\n",
        "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
        "    return sum([get_log_prob_addk(word, unigram_counts,k) for word in sentence])\n",
        "\n",
        "\n",
        "def calculate_perplexity_uni(sentences,unigram_counts, token_count, k):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in sentences:\n",
        "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
        "        total_log_prob += get_sent_log_prob_addk(sentence,unigram_counts,k)\n",
        "    return math.exp(-total_log_prob/test_token_count)\n",
        "\n",
        "\n",
        "f = open(NEWSTEST_ENG)\n",
        "\n",
        "test_sents = []\n",
        "for line in f:\n",
        "    test_sents.append(tokenize(line))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7a8tmeuvb6i",
        "outputId": "dca87e47-4145-4e8d-80e0-608cc350479a"
      },
      "outputs": [],
      "source": [
        "#Calculating the perplexity for different ks\n",
        "ks = [0.0001,0.01,0.1,1,10]\n",
        "\n",
        "for k in ks:\n",
        "    print(str(k) +\": \" + str(calculate_perplexity_uni(test_sents,unigram_counts,token_count,k)))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70H_RmaVw1qu"
      },
      "outputs": [],
      "source": [
        "#Calculating the N1/N paramaters for Trigrams/Bigrams/Unigrams in Katz-Backoff Smoothing\n",
        "\n",
        "TRI_ONES = 0 \n",
        "TRI_TOTAL = 0 \n",
        "\n",
        "for twod in trigram_counts.values():\n",
        "    for oned in twod.values():\n",
        "        for val in oned.values():\n",
        "            if val==1:\n",
        "                TRI_ONES+=1 \n",
        "            TRI_TOTAL += 1 \n",
        "\n",
        "BI_ONES = 0 \n",
        "BI_TOTAL = 0 \n",
        "\n",
        "for oned in bigram_counts.values():\n",
        "    for val in oned.values():\n",
        "        if val==1:\n",
        "            BI_ONES += 1 \n",
        "        BI_TOTAL += 1 \n",
        "        \n",
        "UNI_ONES = list(unigram_counts.values()).count(1)\n",
        "UNI_TOTAL = len(unigram_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Ixb_m4w5uI"
      },
      "outputs": [],
      "source": [
        "#Constructing trigram model with backoff smoothing\n",
        "\n",
        "TRI_ALPHA = TRI_ONES/TRI_TOTAL #Alpha parameter for trigram counts\n",
        "    \n",
        "BI_ALPHA = BI_ONES/BI_TOTAL #Alpha parameter for bigram counts\n",
        "\n",
        "UNI_ALPHA = UNI_ONES/UNI_TOTAL\n",
        "    \n",
        "def get_log_prob_back(sentence,i,unigram_counts,bigram_counts,trigram_counts,token_count):\n",
        "    if trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i],0) > 0:\n",
        "        return math.log((1-TRI_ALPHA)*trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i])/bigram_counts[sentence[i-2]][sentence[i-1]])\n",
        "    else:\n",
        "        if bigram_counts[sentence[i-1]].get(sentence[i],0)>0:\n",
        "            return math.log(TRI_ALPHA*((1-BI_ALPHA)*bigram_counts[sentence[i-1]][sentence[i]]/unigram_counts[sentence[i-1]]))\n",
        "        else:\n",
        "            return math.log(TRI_ALPHA*BI_ALPHA*(1-UNI_ALPHA)*((unigram_counts[sentence[i]]+0.0001)/(token_count+(0.0001)*len(unigram_counts)))) \n",
        "        \n",
        "        \n",
        "def get_sent_log_prob_back(sentence, unigram_counts, bigram_counts,trigram_counts, token_count):\n",
        "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
        "    return sum([get_log_prob_back(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count) for i in range(2,len(sentence))])\n",
        "\n",
        "\n",
        "def calculate_perplexity_tri(sentences,unigram_counts,bigram_counts,trigram_counts, token_count):\n",
        "    total_log_prob = 0\n",
        "    test_token_count = 0\n",
        "    for sentence in sentences:\n",
        "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
        "        total_log_prob += get_sent_log_prob_back(sentence,unigram_counts,bigram_counts,trigram_counts,token_count)\n",
        "    return math.exp(-total_log_prob/test_token_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy_iu_ZIzXVi",
        "outputId": "edd12b69-0318-4b31-ffe8-6472766934b5"
      },
      "outputs": [],
      "source": [
        "#Calculating the perplexity \n",
        "calculate_perplexity_tri(test_sents,unigram_counts,bigram_counts,trigram_counts,token_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOAjqgrGmZwp"
      },
      "outputs": [],
      "source": [
        "#Creating lists of English and German sentences from bitext.\n",
        "\n",
        "from nltk.translate import IBMModel1\n",
        "from nltk.translate import AlignedSent, Alignment\n",
        "\n",
        "eng_sents = []\n",
        "de_sents = []\n",
        "\n",
        "f = open(BITEXT_ENG)\n",
        "for line in f:\n",
        "    terms = tokenize(line)\n",
        "    eng_sents.append(terms)\n",
        "f.close()\n",
        "\n",
        "f = open(BITEXT_DE)\n",
        "for line in f:\n",
        "    terms = tokenize(line)\n",
        "    de_sents.append(terms)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWk_7XXMmdmS"
      },
      "outputs": [],
      "source": [
        "#Zipping together the bitexts for easier access\n",
        "paral_sents = list(zip(eng_sents,de_sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHUJO1pwmfI7",
        "outputId": "9a243997-91f2-4f4d-9fe7-8834f76380fb"
      },
      "outputs": [],
      "source": [
        "print(paral_sents[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEDcDyW6nI4W",
        "outputId": "d81ad3f1-349b-4c82-f6b3-422a2c1a4134"
      },
      "outputs": [],
      "source": [
        "print(eng_sents[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xTZnfClndoL"
      },
      "outputs": [],
      "source": [
        "#Building English to German translation table for words (Forward alignment)\n",
        "eng_de_bt = [AlignedSent(E,G) for E,G in paral_sents]\n",
        "eng_de_m = IBMModel1(eng_de_bt, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRP0Hl0hoWiY"
      },
      "outputs": [],
      "source": [
        "#Building German to English translation table for words (Backward alignment)\n",
        "de_eng_bt = [AlignedSent(G,E) for E,G in paral_sents]\n",
        "de_eng_m = IBMModel1(de_eng_bt, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ0i_SGkt1tu"
      },
      "outputs": [],
      "source": [
        "#Script below to combine alignments using set intersections\n",
        "combined_align = []\n",
        "\n",
        "for i in range(len(eng_de_bt)):\n",
        "\n",
        "    forward = {x for x in eng_de_bt[i].alignment}\n",
        "    back_reversed = {x[::-1] for x in de_eng_bt[i].alignment}\n",
        "    \n",
        "    combined_align.append(forward.intersection(back_reversed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb7JrCFHwHqG"
      },
      "outputs": [],
      "source": [
        "#Creating German to English dictionary with occurence count of word pairs\n",
        "de_eng_count = defaultdict(dict)\n",
        "\n",
        "for i in range(len(de_eng_bt)):\n",
        "    for item in combined_align[i]:\n",
        "        de_eng_count[de_eng_bt[i].words[item[1]]][de_eng_bt[i].mots[item[0]]] =  de_eng_count[de_eng_bt[i].words[item[1]]].get(de_eng_bt[i].mots[item[0]],0) + 1\n",
        "\n",
        "#Creating a English to German dict with occ count of word pais\n",
        "eng_de_count = defaultdict(dict)\n",
        "\n",
        "for i in range(len(eng_de_bt)):\n",
        "    for item in combined_align[i]:\n",
        "        eng_de_count[eng_de_bt[i].words[item[0]]][eng_de_bt[i].mots[item[1]]] =  eng_de_count[eng_de_bt[i].words[item[0]]].get(eng_de_bt[i].mots[item[1]],0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD0oQqgRzeH3"
      },
      "outputs": [],
      "source": [
        "#Creating German to English table with word translation probabilities          \n",
        "de_eng_prob = defaultdict(dict)\n",
        "\n",
        "for de in de_eng_count.keys():\n",
        "    for eng in de_eng_count[de].keys():\n",
        "        de_eng_prob[de][eng] = de_eng_count[de][eng]/sum(de_eng_count[de].values())\n",
        "\n",
        "#Creating English to German dict with word translation probabilities \n",
        "eng_de_prob = defaultdict(dict)\n",
        "\n",
        "for eng in eng_de_count.keys():\n",
        "    for de in eng_de_count[eng].keys():\n",
        "        eng_de_prob[eng][de] = eng_de_count[eng][de]/sum(eng_de_count[eng].values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnd_sjsK4y8G",
        "outputId": "ab85f445-d54d-45c8-b389-b2e8ff0c0f6e"
      },
      "outputs": [],
      "source": [
        "i=9\n",
        "for de in de_eng_count.items():\n",
        "  print(de)\n",
        "  if i<1:\n",
        "    break\n",
        "  i=i-1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBttGuxU4717",
        "outputId": "03d6ee43-c074-4a54-a7af-59018ad85fd5"
      },
      "outputs": [],
      "source": [
        "i=9\n",
        "for de in eng_de_count.items():\n",
        "  print(de)\n",
        "  if i<1:\n",
        "    break\n",
        "  i=i-1\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pfSrw-T8Rn7",
        "outputId": "54799b33-f70b-43ce-ebd5-532208503434"
      },
      "outputs": [],
      "source": [
        "#Examples of translating individual words from German to English\n",
        "print(de_eng_prob['frage'])\n",
        "\n",
        "print(de_eng_prob['nie'])\n",
        "\n",
        "print(de_eng_prob['haus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSz8-3FZ-l-s"
      },
      "outputs": [],
      "source": [
        "#Building noisy channel translation model\n",
        "def de_eng_noisy(german):\n",
        "    noisy={}\n",
        "    for eng in de_eng_prob[german].keys():\n",
        "        noisy[eng] = eng_de_prob[eng][german]+ get_log_prob_addk(eng,unigram_counts,0.0001)\n",
        "    return noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHGZMRke_8qp",
        "outputId": "caf19d0b-dabd-4920-a41a-d933f8382106"
      },
      "outputs": [],
      "source": [
        "#Test block to check alignments\n",
        "print(de_eng_noisy('nie'))\n",
        "print(de_eng_noisy('haus'))\n",
        "print(de_eng_noisy('das'))\n",
        "print(de_eng_noisy('entschuldigung'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOyBQbX2AplO",
        "outputId": "7bb45224-994f-40f1-81e5-4143a1ad03de"
      },
      "outputs": [],
      "source": [
        "eng_de_prob['father']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY3xiVL9B1WL",
        "outputId": "12a78397-94b9-4a06-d420-0348d91b7769"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Function for direct translation\n",
        "def de_eng_direct(query):\n",
        "    query_english = [] \n",
        "    query_tokens = tokenize(query)\n",
        "    \n",
        "    for token in query_tokens:\n",
        "        try:\n",
        "            query_english.append(max(de_eng_prob[token], key=de_eng_prob[token].get))\n",
        "        except:\n",
        "            query_english.append(token) \n",
        "    return \" \".join(query_english)\n",
        "\n",
        "#Function for noisy channel translation\n",
        "def de_eng_noisy_translate(query):  \n",
        "    query_english = [] \n",
        "    query_tokens = tokenize(query)\n",
        "    \n",
        "    for token in query_tokens:\n",
        "        try:\n",
        "            query_english.append(max(de_eng_noisy(token), key=de_eng_noisy(token).get))\n",
        "        except:\n",
        "            query_english.append(token) \n",
        "    return \" \".join(query_english)\n",
        "            \n",
        "f = open(DEVELOPMENT_QUERIES)\n",
        "\n",
        "lno = 0\n",
        "plno = 0\n",
        "\n",
        "#Also building a dictionary of query ids and query content \n",
        "german_qs = {}\n",
        "\n",
        "test_query_trans_sents = [] \n",
        "\n",
        "for line in f:\n",
        "    lno+=1\n",
        "    query_id = line.split('\\t')[0]\n",
        "    query_german = line.split('\\t')[1]  \n",
        "    \n",
        "    german_qs[query_id] = query_german.strip()\n",
        "    \n",
        "    translation = str(de_eng_noisy_translate(query_german))\n",
        " \n",
        "    if plno<5:\n",
        "        print(query_id + \"\\n\" + \"German: \" + str(query_german) + \"\\n\" + \"English: \" + translation +\"\\n\\n\")\n",
        "        plno+=1\n",
        "    test_query_trans_sents.append(translation)\n",
        "    if lno==100:\n",
        "        break\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GevO1c3AJzUX",
        "outputId": "1cc35b3f-daa5-4a4d-ca38-50c25faede9d"
      },
      "outputs": [],
      "source": [
        "translation = str(de_eng_noisy_translate(\"welches ist das höchste gebäude der welt?\"))\n",
        "print(translation)\n",
        "retr_docs(translation,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULhf8VmaKy6Q",
        "outputId": "be488c9f-1f75-4408-c183-ff803ae7d0ae"
      },
      "outputs": [],
      "source": [
        "f = open(DEVELOPMENT_DOCS)\n",
        "\n",
        "for line in f:\n",
        "    doc = line.split(\"\\t\")\n",
        "    if doc[0]=='63341':\n",
        "      print(doc[1])\n",
        "      break\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjJjwVbeflC9",
        "outputId": "c986fb60-51f7-40d5-cd91-821a296d2238"
      },
      "outputs": [],
      "source": [
        "translation = str(de_eng_direct(\"welches ist das höchste gebäude der welt?\"))\n",
        "print(translation)\n",
        "retr_docs(translation,5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "7b47566efb25eea7c5a36f8d4e14e44cbbde08c492e229843ca267b4ef9a6c79"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
