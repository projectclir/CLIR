{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JXW24tIuTgHh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVcFTv2JOyBy"
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKCNi8rwN8yf",
    "outputId": "63f79fbe-c620-41d1-946a-15cd39e51065"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kalai-\n",
      "[nltk_data]     pt6931/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/kalai-\n",
      "[nltk_data]     pt6931/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.lower()\n",
    "    return [token for token in tokenizer(utf_line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwK8O4NQcTdk"
   },
   "source": [
    "**Example for tokenization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR7ezD4NO88s",
    "outputId": "4bcdae7f-06d0-413e-b498-9cbea5ee5622"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kalai-\n",
      "[nltk_data]     pt6931/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/kalai-\n",
      "[nltk_data]     pt6931/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nichts', 'ist', ',', 'wie', 'es', 'scheint', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions import tokenize\n",
    "tokenize(\"Nichts ist, wie es scheint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEQtLgdJcx4n"
   },
   "source": [
    "**Naming the path of the data files as variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7GGpNF2tTdc5"
   },
   "outputs": [],
   "source": [
    "DEVELOPMENT_DOCS = 'data/devel.docs' \n",
    "\n",
    "DEVELOPMENT_QUERIES = 'data/devel.queries' \n",
    "\n",
    "DEVELOPMENT_QREL = 'data/devel.qrel' \n",
    "\n",
    "BITEXT_ENG = 'data/bitext.en' \n",
    "\n",
    "BITEXT_DE = 'data/bitext.de' \n",
    "\n",
    "NEWSTEST_ENG = 'data/newstest.en' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGlLPO2bgIN9"
   },
   "source": [
    "**Loading the devel.docs file, extracting and tokenizing the terms, and storing them in a python dictionary with the document ids as keys.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j1ukayfnZ-51"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "def extract_and_tokenize_terms(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in stopwords: \n",
    "            if not re.search(r'\\d',token) and not re.search(r'[^A-Za-z-]',token): #Removing numbers and punctuations \n",
    "                terms.append(stemmer.stem(token.lower()))\n",
    "    return terms\n",
    "\n",
    "documents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YnTmkRdVwLeA",
    "outputId": "f5801fcd-1e3e-42d6-adc1-4be9cbcf547a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\tanarchism anarchism is a political philosophy that advocates stateless societies based on non hierarchical free associations anarchism holds the state to be undesirable unnecessary or harmful the following sources cite anarchism as a political philosophy slevin carl anarchism the concise oxford dictionary of politics ed iain mclean and alistair mcmillan oxford university press 2003 while anti statism is central some argue that anarchism entails opposing authority or hierarchical organization in the conduct of human relations including but not limited to the state system as a subtle and anti dogmatic philosophy anarchism draws on many currents of thought and strategy anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy there are many types and traditions of anarchism not all of which are mutually exclusive anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications ostergaard geoffrey anarchism the blackwell dictionary of modern social thought blackwell publishing p. 14. anarchism is often considered a radical left wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics anarchism as a mass social movement has regularly endured fluctuations in popularity the central tendency of anarchism as a social movement has been represented by anarcho communism and anarcho syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organizations many anarchists oppose all forms of aggression supporting self defense or non violence anarcho pacifism george woodcock anarchism a history of libertarian ideas and movements 1962 while others have supported the use of some coercive measures including violent revolution and propaganda of the deed on the path to an anarchist society etymology and terminology the term is a compound word composed from the word anarchy and the suffix\n",
      "\n",
      "['12', 'anarchism anarchism is a political philosophy that advocates stateless societies based on non hierarchical free associations anarchism holds the state to be undesirable unnecessary or harmful the following sources cite anarchism as a political philosophy slevin carl anarchism the concise oxford dictionary of politics ed iain mclean and alistair mcmillan oxford university press 2003 while anti statism is central some argue that anarchism entails opposing authority or hierarchical organization in the conduct of human relations including but not limited to the state system as a subtle and anti dogmatic philosophy anarchism draws on many currents of thought and strategy anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy there are many types and traditions of anarchism not all of which are mutually exclusive anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications ostergaard geoffrey anarchism the blackwell dictionary of modern social thought blackwell publishing p. 14. anarchism is often considered a radical left wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics anarchism as a mass social movement has regularly endured fluctuations in popularity the central tendency of anarchism as a social movement has been represented by anarcho communism and anarcho syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organizations many anarchists oppose all forms of aggression supporting self defense or non violence anarcho pacifism george woodcock anarchism a history of libertarian ideas and movements 1962 while others have supported the use of some coercive measures including violent revolution and propaganda of the deed on the path to an anarchist society etymology and terminology the term is a compound word composed from the word anarchy and the suffix\\n']\n",
      "anarchism anarchism is a political philosophy that advocates stateless societies based on non hierarchical free associations anarchism holds the state to be undesirable unnecessary or harmful the following sources cite anarchism as a political philosophy slevin carl anarchism the concise oxford dictionary of politics ed iain mclean and alistair mcmillan oxford university press 2003 while anti statism is central some argue that anarchism entails opposing authority or hierarchical organization in the conduct of human relations including but not limited to the state system as a subtle and anti dogmatic philosophy anarchism draws on many currents of thought and strategy anarchism does not offer a fixed body of doctrine from a single particular world view instead fluxing and flowing as a philosophy there are many types and traditions of anarchism not all of which are mutually exclusive anarchist schools of thought can differ fundamentally supporting anything from extreme individualism to complete collectivism strains of anarchism have often been divided into the categories of social and individualist anarchism or similar dual classifications ostergaard geoffrey anarchism the blackwell dictionary of modern social thought blackwell publishing p. 14. anarchism is often considered a radical left wing ideology and much of anarchist economics and anarchist legal philosophy reflect anti authoritarian interpretations of communism collectivism syndicalism mutualism or participatory economics anarchism as a mass social movement has regularly endured fluctuations in popularity the central tendency of anarchism as a social movement has been represented by anarcho communism and anarcho syndicalism with individualist anarchism being primarily a literary phenomenon which nevertheless did have an impact on the bigger currents and individualists have also participated in large anarchist organizations many anarchists oppose all forms of aggression supporting self defense or non violence anarcho pacifism george woodcock anarchism a history of libertarian ideas and movements 1962 while others have supported the use of some coercive measures including violent revolution and propaganda of the deed on the path to an anarchist society etymology and terminology the term is a compound word composed from the word anarchy and the suffix\n",
      "\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "f = open(DEVELOPMENT_DOCS)\n",
    "\n",
    "for line in f:\n",
    "    print(line)\n",
    "    doc = line.split(\"\\t\")\n",
    "    print(doc)\n",
    "    print(doc[1])\n",
    "    print(doc[0])\n",
    "    break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AXJKXjEeYicv"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dr/vc3dbtks4q19f4vxx7lj6r5jq3dhdn/T/ipykernel_19086/2695435261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_and_tokenize_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/dr/vc3dbtks4q19f4vxx7lj6r5jq3dhdn/T/ipykernel_19086/87421867.py\u001b[0m in \u001b[0;36mextract_and_tokenize_terms\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^A-Za-z-]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Removing numbers and punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mterms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step3\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mNESS\u001b[0m  \u001b[0;34m->\u001b[0m                  \u001b[0mgoodness\u001b[0m       \u001b[0;34m->\u001b[0m  \u001b[0mgood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \"\"\"\n\u001b[0;32m--> 530\u001b[0;31m         return self._apply_rule_list(\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             [\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mstem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_has_positive_measure\u001b[0;34m(self, stem)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_contains_vowel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_measure\u001b[0;34m(self, stem)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mcv_sequence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mcv_sequence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open(DEVELOPMENT_DOCS, encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    doc = line.split(\"\\t\")\n",
    "    terms = extract_and_tokenize_terms(doc[1])\n",
    "    documents[doc[0]] = terms\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPd7Uvuobj5L",
    "outputId": "cf5449e0-624b-4270-ea88-e752ebce50f0"
   },
   "outputs": [],
   "source": [
    "documents['308'][:65] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHeZaIoN55h_"
   },
   "source": [
    "**Building an inverted index for the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CSsg4e295anK"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for docid, terms in documents.items():\n",
    "    for term in terms:\n",
    "        inverted_index[term].add(docid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w31mwYbd6BO-",
    "outputId": "4191d70d-a2c7-424a-c151-05e82b813795"
   },
   "outputs": [],
   "source": [
    "sorted(inverted_index['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3uOwZRpH3oF"
   },
   "source": [
    "**Building a TF-IDF representation using BM25**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0UoAKvfwCpuL"
   },
   "outputs": [],
   "source": [
    "NO_DOCS = len(documents) \n",
    "\n",
    "AVG_LEN_DOC = sum([len(doc) for doc in documents.values()])/len(documents) \n",
    "\n",
    "def tf_idf_score(k1,b,term,docid):  \n",
    "    \n",
    "    ft = len(inverted_index[term]) \n",
    "    term = stemmer.stem(term.lower())\n",
    "    fdt =  documents[docid].count(term)\n",
    "    \n",
    "    idf_comp = math.log((NO_DOCS - ft + 0.5)/(ft+0.5))\n",
    "    \n",
    "    tf_comp = ((k1 + 1)*fdt)/(k1*((1-b) + b*(len(documents[docid])/AVG_LEN_DOC))+fdt)\n",
    "    \n",
    "    return idf_comp * tf_comp\n",
    "\n",
    "def create_tf_idf(k1,b):\n",
    "    tf_idf = defaultdict(dict)\n",
    "    for term in set(inverted_index.keys()):\n",
    "        for docid in inverted_index[term]:\n",
    "            tf_idf[term][docid] = tf_idf_score(k1,b,term,docid)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ztWRSQiNHRIu"
   },
   "outputs": [],
   "source": [
    "tf_idf = create_tf_idf(1.6,0.875)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "N1be1LUyQFsU"
   },
   "outputs": [],
   "source": [
    "def get_qtf_comp(k3,term,fqt):\n",
    "    return ((k3+1)*fqt[term])/(k3 + fqt[term])\n",
    "\n",
    "\n",
    "#Function to retrieve documents || Returns a set of documents and their relevance scores. \n",
    "def retr_docs(query,result_count):\n",
    "    q_terms = [stemmer.stem(term.lower()) for term in query.split() if term not in stopwords] \n",
    "    fqt = {}\n",
    "    for term in q_terms:\n",
    "        fqt[term] = fqt.get(term,0) + 1\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for word in fqt.keys():\n",
    "        for document in inverted_index[word]:\n",
    "            scores[document] = scores.get(document,0) + (tf_idf[word][document]*get_qtf_comp(1.5,word,fqt)) \n",
    "    \n",
    "    return sorted(scores.items(),key = lambda x : x[1] , reverse=True)[:result_count] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "woL9RHlGQNsW",
    "outputId": "f03c6360-d03b-45e5-961c-2d3bcfe3a852"
   },
   "outputs": [],
   "source": [
    "retr_docs(\"which is the tallest building in the world?\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiQ07QVtQeLW",
    "outputId": "6ae39ec7-4a09-4b4c-a442-e56294fe8c67"
   },
   "outputs": [],
   "source": [
    "documents['34080'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3NySdS3QyXk",
    "outputId": "1a42b114-a32c-4816-de9d-c82749ea56fd"
   },
   "outputs": [],
   "source": [
    "f = open(DEVELOPMENT_DOCS)\n",
    "\n",
    "for line in f:\n",
    "    doc = line.split(\"\\t\")\n",
    "    if doc[0]=='34080':\n",
    "      print(doc[1])\n",
    "      break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7AbTcNvcLQ5z"
   },
   "outputs": [],
   "source": [
    "#Calculating the unigram, bigram and trigram counts. \n",
    "\n",
    "f = open(BITEXT_ENG, encoding='utf-8')\n",
    "\n",
    "train_sentences = []\n",
    "\n",
    "for line in f:\n",
    "    train_sentences.append(tokenize(line))\n",
    "\n",
    "f.close()    \n",
    "\n",
    "#Function to mark the first occurence of words as unknown, for training.\n",
    "def check_for_unk_train(word,unigram_counts):\n",
    "    if word in unigram_counts:\n",
    "        return word\n",
    "    else:\n",
    "        unigram_counts[word] = 0\n",
    "        return \"UNK\"\n",
    "\n",
    "#Function to convert sentences for training the language model.    \n",
    "def convert_sentence_train(sentence,unigram_counts):\n",
    "    #<s1> and <s2> are sentinel tokens added to the start and end\n",
    "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s2>\"]+ [\"</s1>\"]\n",
    "\n",
    "#Function to obtain unigram, bigram and trigram counts.\n",
    "def get_counts(sentences):\n",
    "    trigram_counts = defaultdict(lambda: defaultdict(dict))\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    unigram_counts = {}\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]] = trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0) + 1\n",
    "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    unigram_counts[\"</s1>\"] = unigram_counts[\"<s1>\"]\n",
    "    unigram_counts[\"</s2>\"] = unigram_counts[\"<s2>\"]\n",
    "    bigram_counts[\"</s2>\"][\"</s1>\"] = bigram_counts[\"<s1>\"][\"<s2>\"]\n",
    "    return unigram_counts, bigram_counts, trigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "j-GWkwo3pHmC"
   },
   "outputs": [],
   "source": [
    "unigram_counts, bigram_counts,trigram_counts = get_counts(train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "m0wc-D2IpsV8"
   },
   "outputs": [],
   "source": [
    "#Constructing unigram model with 'add-k' smoothing\n",
    "token_count = sum(unigram_counts.values())\n",
    "\n",
    "#Function to convert unknown words for testing. \n",
    "def check_for_unk_test(word,unigram_counts):\n",
    "    if word in unigram_counts and unigram_counts[word] > 0:\n",
    "        return word\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "\n",
    "def convert_sentence_test(sentence,unigram_counts):\n",
    "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s2>\"]  + [\"</s1>\"]\n",
    "\n",
    "#Returns the log probability of a unigram, with add-k smoothing.\n",
    "def get_log_prob_addk(word,unigram_counts,k):\n",
    "    return math.log((unigram_counts[word] + k)/ \\\n",
    "                    (token_count + k*len(unigram_counts)))\n",
    "\n",
    "#Returns the log probability of a sentence.\n",
    "def get_sent_log_prob_addk(sentence, unigram_counts,k):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_prob_addk(word, unigram_counts,k) for word in sentence])\n",
    "\n",
    "\n",
    "def calculate_perplexity_uni(sentences,unigram_counts, token_count, k):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += get_sent_log_prob_addk(sentence,unigram_counts,k)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "\n",
    "f = open(NEWSTEST_ENG)\n",
    "\n",
    "test_sents = []\n",
    "for line in f:\n",
    "    test_sents.append(tokenize(line))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7a8tmeuvb6i",
    "outputId": "dca87e47-4145-4e8d-80e0-608cc350479a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001: 631.1001268265437\n",
      "0.01: 631.1751762621166\n",
      "0.1: 631.9125779449058\n",
      "1: 642.807839891329\n",
      "10: 814.3070354176195\n"
     ]
    }
   ],
   "source": [
    "#Calculating the perplexity for different ks\n",
    "ks = [0.0001,0.01,0.1,1,10]\n",
    "\n",
    "for k in ks:\n",
    "    print(str(k) +\": \" + str(calculate_perplexity_uni(test_sents,unigram_counts,token_count,k)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "70H_RmaVw1qu"
   },
   "outputs": [],
   "source": [
    "#Calculating the N1/N paramaters for Trigrams/Bigrams/Unigrams in Katz-Backoff Smoothing\n",
    "\n",
    "TRI_ONES = 0 \n",
    "TRI_TOTAL = 0 \n",
    "\n",
    "for twod in trigram_counts.values():\n",
    "    for oned in twod.values():\n",
    "        for val in oned.values():\n",
    "            if val==1:\n",
    "                TRI_ONES+=1 \n",
    "            TRI_TOTAL += 1 \n",
    "\n",
    "BI_ONES = 0 \n",
    "BI_TOTAL = 0 \n",
    "\n",
    "for oned in bigram_counts.values():\n",
    "    for val in oned.values():\n",
    "        if val==1:\n",
    "            BI_ONES += 1 \n",
    "        BI_TOTAL += 1 \n",
    "        \n",
    "UNI_ONES = list(unigram_counts.values()).count(1)\n",
    "UNI_TOTAL = len(unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "t6Ixb_m4w5uI"
   },
   "outputs": [],
   "source": [
    "#Constructing trigram model with backoff smoothing\n",
    "\n",
    "TRI_ALPHA = TRI_ONES/TRI_TOTAL #Alpha parameter for trigram counts\n",
    "    \n",
    "BI_ALPHA = BI_ONES/BI_TOTAL #Alpha parameter for bigram counts\n",
    "\n",
    "UNI_ALPHA = UNI_ONES/UNI_TOTAL\n",
    "    \n",
    "def get_log_prob_back(sentence,i,unigram_counts,bigram_counts,trigram_counts,token_count):\n",
    "    if trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i],0) > 0:\n",
    "        return math.log((1-TRI_ALPHA)*trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i])/bigram_counts[sentence[i-2]][sentence[i-1]])\n",
    "    else:\n",
    "        if bigram_counts[sentence[i-1]].get(sentence[i],0)>0:\n",
    "            return math.log(TRI_ALPHA*((1-BI_ALPHA)*bigram_counts[sentence[i-1]][sentence[i]]/unigram_counts[sentence[i-1]]))\n",
    "        else:\n",
    "            return math.log(TRI_ALPHA*BI_ALPHA*(1-UNI_ALPHA)*((unigram_counts[sentence[i]]+0.0001)/(token_count+(0.0001)*len(unigram_counts)))) \n",
    "        \n",
    "        \n",
    "def get_sent_log_prob_back(sentence, unigram_counts, bigram_counts,trigram_counts, token_count):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_prob_back(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count) for i in range(2,len(sentence))])\n",
    "\n",
    "\n",
    "def calculate_perplexity_tri(sentences,unigram_counts,bigram_counts,trigram_counts, token_count):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += get_sent_log_prob_back(sentence,unigram_counts,bigram_counts,trigram_counts,token_count)\n",
    "    return math.exp(-total_log_prob/test_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zy_iu_ZIzXVi",
    "outputId": "edd12b69-0318-4b31-ffe8-6472766934b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463.4051799314212"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the perplexity \n",
    "calculate_perplexity_tri(test_sents,unigram_counts,bigram_counts,trigram_counts,token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "YOAjqgrGmZwp"
   },
   "outputs": [],
   "source": [
    "#Creating lists of English and German sentences from bitext.\n",
    "\n",
    "from nltk.translate import IBMModel1\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "eng_sents = []\n",
    "de_sents = []\n",
    "\n",
    "f = open(BITEXT_ENG, encoding='utf-8')\n",
    "for line in f:\n",
    "    terms = tokenize(line)\n",
    "    eng_sents.append(terms)\n",
    "f.close()\n",
    "\n",
    "f = open(BITEXT_DE, encoding = 'utf-8')\n",
    "for line in f:\n",
    "    terms = tokenize(line)\n",
    "    de_sents.append(terms)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "tWk_7XXMmdmS"
   },
   "outputs": [],
   "source": [
    "#Zipping together the bitexts for easier access\n",
    "paral_sents = list(zip(eng_sents,de_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHUJO1pwmfI7",
    "outputId": "9a243997-91f2-4f4d-9fe7-8834f76380fb"
   },
   "outputs": [],
   "source": [
    "print(paral_sents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEDcDyW6nI4W",
    "outputId": "d81ad3f1-349b-4c82-f6b3-422a2c1a4134"
   },
   "outputs": [],
   "source": [
    "print(eng_sents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "3xTZnfClndoL"
   },
   "outputs": [],
   "source": [
    "#Building English to German translation table for words (Forward alignment)\n",
    "eng_de_bt = [AlignedSent(E,G) for E,G in paral_sents]\n",
    "eng_de_m = IBMModel1(eng_de_bt, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "LRP0Hl0hoWiY"
   },
   "outputs": [],
   "source": [
    "#Building German to English translation table for words (Backward alignment)\n",
    "de_eng_bt = [AlignedSent(G,E) for E,G in paral_sents]\n",
    "de_eng_m = IBMModel1(de_eng_bt, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "QQ0i_SGkt1tu"
   },
   "outputs": [],
   "source": [
    "#Script below to combine alignments using set intersections\n",
    "combined_align = []\n",
    "\n",
    "for i in range(len(eng_de_bt)):\n",
    "\n",
    "    forward = {x for x in eng_de_bt[i].alignment}\n",
    "    back_reversed = {x[::-1] for x in de_eng_bt[i].alignment}\n",
    "    \n",
    "    combined_align.append(forward.intersection(back_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Xb7JrCFHwHqG"
   },
   "outputs": [],
   "source": [
    "#Creating German to English dictionary with occurence count of word pairs\n",
    "de_eng_count = defaultdict(dict)\n",
    "\n",
    "for i in range(len(de_eng_bt)):\n",
    "    for item in combined_align[i]:\n",
    "        de_eng_count[de_eng_bt[i].words[item[1]]][de_eng_bt[i].mots[item[0]]] =  de_eng_count[de_eng_bt[i].words[item[1]]].get(de_eng_bt[i].mots[item[0]],0) + 1\n",
    "\n",
    "#Creating a English to German dict with occ count of word pais\n",
    "eng_de_count = defaultdict(dict)\n",
    "\n",
    "for i in range(len(eng_de_bt)):\n",
    "    for item in combined_align[i]:\n",
    "        eng_de_count[eng_de_bt[i].words[item[0]]][eng_de_bt[i].mots[item[1]]] =  eng_de_count[eng_de_bt[i].words[item[0]]].get(eng_de_bt[i].mots[item[1]],0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "WD0oQqgRzeH3"
   },
   "outputs": [],
   "source": [
    "#Creating German to English table with word translation probabilities          \n",
    "de_eng_prob = defaultdict(dict)\n",
    "\n",
    "for de in de_eng_count.keys():\n",
    "    for eng in de_eng_count[de].keys():\n",
    "        de_eng_prob[de][eng] = de_eng_count[de][eng]/sum(de_eng_count[de].values())\n",
    "\n",
    "#Creating English to German dict with word translation probabilities \n",
    "eng_de_prob = defaultdict(dict)\n",
    "\n",
    "for eng in eng_de_count.keys():\n",
    "    for de in eng_de_count[eng].keys():\n",
    "        eng_de_prob[eng][de] = eng_de_count[eng][de]/sum(eng_de_count[eng].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnd_sjsK4y8G",
    "outputId": "ab85f445-d54d-45c8-b389-b2e8ff0c0f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10.000', {'10,000': 1, 'waited': 1})\n",
      "('gold', {'gold': 5, 'ites': 1})\n",
      "('?', {'?': 388})\n",
      "('dollar', {'$': 4, 'dollar': 7})\n",
      "('zu', {'to': 668, ',': 2, 'it': 2})\n",
      "('san', {'san': 3, 'border': 1})\n",
      "('.', {'.': 15769})\n",
      "('den', {'the': 186, 's': 9, 'to': 29, 'in': 2})\n",
      "('es', {'it': 281, 'is': 66, 'not': 25, 'to': 3, 'there': 2})\n",
      "('nie', {'never': 22})\n"
     ]
    }
   ],
   "source": [
    "i=9\n",
    "for de in de_eng_count.items():\n",
    "  print(de)\n",
    "  if i<1:\n",
    "    break\n",
    "  i=i-1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBttGuxU4717",
    "outputId": "03d6ee43-c074-4a54-a7af-59018ad85fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('10,000', {'10.000': 1, 'lebhafte': 1, 'festzustellen': 1})\n",
      "('gold', {'gold': 5})\n",
      "('?', {'?': 388, 'kim': 1, 'wie': 1})\n",
      "('$', {'dollar': 4})\n",
      "('to', {'zu': 668, 'von': 9, ',': 169, 'den': 29, 'für': 6, 'auf': 2, 'es': 3, 'werden': 4, 'sich': 4, 'um': 1, 'sie': 2})\n",
      "('san', {'san': 3})\n",
      "('.', {'.': 15769, 'unaufhörlich': 1, 'zurückgeben': 1, 'tripolis': 1})\n",
      "('the', {'den': 186, 'die': 4601, 'der': 1236, 'das': 91, 'des': 14, 'dem': 10, 'im': 9})\n",
      "('it', {'es': 281, 'sie': 59, 'hat': 1, 'ist': 9, 'nicht': 3, 'er': 1, 'dasselbe': 1, 'zu': 2, 'mehr': 1, 'wird': 1, 'mithalten': 1, 'zugewanderte': 1})\n",
      "('never', {'nie': 22, 'gesund': 1})\n"
     ]
    }
   ],
   "source": [
    "i=9\n",
    "for de in eng_de_count.items():\n",
    "  print(de)\n",
    "  if i<1:\n",
    "    break\n",
    "  i=i-1\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pfSrw-T8Rn7",
    "outputId": "54799b33-f70b-43ce-ebd5-532208503434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 1.0}\n",
      "{'never': 1.0}\n",
      "{'house': 0.625, 'charity': 0.125, 'hospitalized': 0.125, 'offset': 0.125}\n"
     ]
    }
   ],
   "source": [
    "#Examples of translating individual words from German to English\n",
    "print(de_eng_prob['frage'])\n",
    "\n",
    "print(de_eng_prob['nie'])\n",
    "\n",
    "print(de_eng_prob['haus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "OSz8-3FZ-l-s"
   },
   "outputs": [],
   "source": [
    "#Building noisy channel translation model\n",
    "def de_eng_noisy(german):\n",
    "    noisy={}\n",
    "    for eng in de_eng_prob[german].keys():\n",
    "        noisy[eng] = eng_de_prob[eng][german]+ get_log_prob_addk(eng,unigram_counts,0.0001)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHGZMRke_8qp",
    "outputId": "caf19d0b-dabd-4920-a41a-d933f8382106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'never': -7.034609059797315}\n",
      "{'house': -8.153224281882322, 'charity': -11.032065847914977, 'hospitalized': -11.319739587276473, 'offset': -11.225188029412386}\n",
      "{'this': -4.991400947456017, 'that': -4.857303935975769, 'is': -4.278625574868814, 'the': -3.0973206970208214, 'for': -5.074260930518287, 'of': -3.88129895540149}\n",
      "{'pledging': -21.128725580698557}\n"
     ]
    }
   ],
   "source": [
    "#Test block to check alignments\n",
    "print(de_eng_noisy('nie'))\n",
    "print(de_eng_noisy('haus'))\n",
    "print(de_eng_noisy('das'))\n",
    "print(de_eng_noisy('entschuldigung'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOyBQbX2AplO",
    "outputId": "7bb45224-994f-40f1-81e5-4143a1ad03de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_de_prob['father']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tY3xiVL9B1WL",
    "outputId": "12a78397-94b9-4a06-d420-0348d91b7769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "German: der ( von engl . action : tat , handlung , bewegung ) ist ein filmgenre des unterhaltungskinos , in welchem der fortgang der äußeren handlung von zumeist spektakulär inszenierten kampf - und gewaltszenen vorangetrieben und illustriert wird .\n",
      "\n",
      "English: the ( , leninism . action : rattling , side , movement ) is a filmgenre the unterhaltungskinos , in paulson the fortgang the trumpet side , zumeist spektakulär inszenierten fight - and gewaltszenen annan and illustriert is .\n",
      "\n",
      "\n",
      "116\n",
      "German: die ( einheitenzeichen : u für unified atomic mass unit , veraltet amu für atomic mass unit ) ist eine maßeinheit der masse .\n",
      "\n",
      "English: the ( einheitenzeichen : u for unified atomic manipulation unit , regime amu for atomic manipulation unit ) is a befuddled the masse .\n",
      "\n",
      "\n",
      "240\n",
      "German: der von lateinisch actualis , \" wirklich \" , auch aktualitätsprinzip , uniformitäts - oder gleichförmigkeitsprinzip , englisch uniformitarianism , ist die grundlegende wissenschaftliche methode in der .\n",
      "\n",
      "English: the , lateinisch actualis , `` really `` , not aktualitätsprinzip , uniformitäts - or gleichförmigkeitsprinzip , english uniformitarianism , is the basic intended method in the .\n",
      "\n",
      "\n",
      "320\n",
      "German: die ( griechisch el , von altgriechisch grc , - \" zusammen - \" , \" anbinden \" , gemeint ist \" die herzbeutel angehängte \" ) , ist ein blutgefäß , welches das blut vom herz wegführt .\n",
      "\n",
      "English: the ( griechisch el , , altgriechisch grc , - `` zusammen - `` , `` anbinden `` , meant is `` the herzbeutel angehängte `` ) , is a blutgefäß , welches the blood vom herz wegführt .\n",
      "\n",
      "\n",
      "540\n",
      "German: unter der bezeichnung fasst man die drei im nördlichen alpenvorland liegenden gewässereinheiten obersee , untersee und seerhein zusammen .\n",
      "\n",
      "English: under the eurasian fasst man the three the nördlichen alpenvorland underlying gewässereinheiten obersee , untersee and seerhein zusammen .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Function for direct translation\n",
    "def de_eng_direct(query):\n",
    "    query_english = [] \n",
    "    query_tokens = tokenize(query)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            query_english.append(max(de_eng_prob[token], key=de_eng_prob[token].get))\n",
    "        except:\n",
    "            query_english.append(token) \n",
    "    return \" \".join(query_english)\n",
    "\n",
    "#Function for noisy channel translation\n",
    "def de_eng_noisy_translate(query):  \n",
    "    query_english = [] \n",
    "    query_tokens = tokenize(query)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            query_english.append(max(de_eng_noisy(token), key=de_eng_noisy(token).get))\n",
    "        except:\n",
    "            query_english.append(token) \n",
    "    return \" \".join(query_english)\n",
    "            \n",
    "f = open(DEVELOPMENT_QUERIES, encoding='utf-8')\n",
    "\n",
    "lno = 0\n",
    "plno = 0\n",
    "\n",
    "#Also building a dictionary of query ids and query content \n",
    "german_qs = {}\n",
    "\n",
    "test_query_trans_sents = [] \n",
    "\n",
    "for line in f:\n",
    "    lno+=1\n",
    "    query_id = line.split('\\t')[0]\n",
    "    query_german = line.split('\\t')[1]  \n",
    "    \n",
    "    german_qs[query_id] = query_german.strip()\n",
    "    \n",
    "    translation = str(de_eng_noisy_translate(query_german))\n",
    " \n",
    "    if plno<5:\n",
    "        print(query_id + \"\\n\" + \"German: \" + str(query_german) + \"\\n\" + \"English: \" + translation +\"\\n\\n\")\n",
    "        plno+=1\n",
    "    test_query_trans_sents.append(translation)\n",
    "    if lno==100:\n",
    "        break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GevO1c3AJzUX",
    "outputId": "1cc35b3f-daa5-4a4d-ca38-50c25faede9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welches is the notions edifice the world ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('63341', 10.4405268930542),\n",
       " ('49702', 10.319217561251946),\n",
       " ('245990', 10.094533935342444),\n",
       " ('91031', 10.084863853266132),\n",
       " ('6042', 8.852462540069059)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = str(de_eng_noisy_translate(\"welches ist das höchste gebäude der welt?\"))\n",
    "print(translation)\n",
    "retr_docs(translation,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULhf8VmaKy6Q",
    "outputId": "be488c9f-1f75-4408-c183-ff803ae7d0ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body with regard to living things a body is the physical body of an individual body often is used in connection with appearance health issues and death the study of the workings of the body is physiology human body the human body mostly consists of a head neck torso two arms and two legs as well as numerous internal organ groups such as respiratory circulatory and a central nervous system variations the dead body of a human is referred to as a cadaver or corpse the dead bodies of vertebrate animals insects and humans are sometimes called carcasses the study of the structure of the body is called anatomy a carcase is the body of a slaughtered animal after the removal of offal that is to be used as meat antonymin the views emerging from the mind body dichotomy the body is considered in behavior and therefore considered as little valued the mind body problem by robert m. young and trivial in comparison to mind spirit or soul materialist philosophers of mind maintain that the mind is not something separate from the body but is produced by physiological functions of the brain see also anatomy antibody battery body art body flexibility bodily harm body image body metaphysics body language body painting disability disease emergence general fitness training healing health human physical appearance human body microtrauma physical body trauma regarding corpses autopsy body farm burial cremation dead bodies and health risks death embalming mummy necrophilia references\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(DEVELOPMENT_DOCS, encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    doc = line.split(\"\\t\")\n",
    "    if doc[0]=='3788':\n",
    "      print(doc[1])\n",
    "      break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjJjwVbeflC9",
    "outputId": "c986fb60-51f7-40d5-cd91-821a296d2238"
   },
   "outputs": [],
   "source": [
    "translation = str(de_eng_direct(\"welches ist das höchste gebäude der welt?\"))\n",
    "print(translation)\n",
    "retr_docs(translation,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Documents']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(documents,'Documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TF-IDF']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tf_idf,'TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EngSents']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(eng_sents, 'EngSents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DeSents']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(de_sents,'DeSents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function IBMModel.reset_probabilities.<locals>.<lambda> at 0x0000023AE1AEA950>: it's not found as nltk.translate.ibm_model.IBMModel.reset_probabilities.<locals>.<lambda>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joblib\u001b[39m.\u001b[39;49mdump(eng_de_m, \u001b[39m'\u001b[39;49m\u001b[39mE2D-Model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:482\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39melif\u001b[39;00m is_filename:\n\u001b[0;32m    481\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 482\u001b[0m         NumpyPickler(f, protocol\u001b[39m=\u001b[39;49mprotocol)\u001b[39m.\u001b[39;49mdump(value)\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     NumpyPickler(filename, protocol\u001b[39m=\u001b[39mprotocol)\u001b[39m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     save(func)\n\u001b[1;32m--> 692\u001b[0m     save(args)\n\u001b[0;32m    693\u001b[0m     write(REDUCE)\n\u001b[0;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[1;32m--> 887\u001b[0m         save(element)\n\u001b[0;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:1071\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     obj2, parent \u001b[39m=\u001b[39m _getattribute(module, name)\n\u001b[0;32m   1070\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mImportError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m):\n\u001b[1;32m-> 1071\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\n\u001b[0;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m: it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms not found as \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1073\u001b[0m         (obj, module_name, name)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1074\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[39mif\u001b[39;00m obj2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m obj:\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function IBMModel.reset_probabilities.<locals>.<lambda> at 0x0000023AE1AEA950>: it's not found as nltk.translate.ibm_model.IBMModel.reset_probabilities.<locals>.<lambda>"
     ]
    }
   ],
   "source": [
    "joblib.dump(eng_de_m, 'E2D-Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function IBMModel.reset_probabilities.<locals>.<lambda> at 0x0000023AE5A20940>: it's not found as nltk.translate.ibm_model.IBMModel.reset_probabilities.<locals>.<lambda>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joblib\u001b[39m.\u001b[39;49mdump(de_eng_m, \u001b[39m'\u001b[39;49m\u001b[39mD2E-Model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:482\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39melif\u001b[39;00m is_filename:\n\u001b[0;32m    481\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 482\u001b[0m         NumpyPickler(f, protocol\u001b[39m=\u001b[39;49mprotocol)\u001b[39m.\u001b[39;49mdump(value)\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     NumpyPickler(filename, protocol\u001b[39m=\u001b[39mprotocol)\u001b[39m.\u001b[39mdump(value)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mstart_framing()\n\u001b[1;32m--> 487\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave(obj)\n\u001b[0;32m    488\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(STOP)\n\u001b[0;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframer\u001b[39m.\u001b[39mend_framing()\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[39mif\u001b[39;00m state_setter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         save(state)\n\u001b[0;32m    718\u001b[0m         write(BUILD)\n\u001b[0;32m    719\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    720\u001b[0m         \u001b[39m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[0;32m    721\u001b[0m         \u001b[39m# to update obj's with its previous state.\u001b[39;00m\n\u001b[0;32m    722\u001b[0m         \u001b[39m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[0;32m    723\u001b[0m         \u001b[39m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite(MARK \u001b[39m+\u001b[39m DICT)\n\u001b[0;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemoize(obj)\n\u001b[1;32m--> 972\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_setitems(obj\u001b[39m.\u001b[39;49mitems())\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    996\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m tmp:\n\u001b[0;32m    997\u001b[0m         save(k)\n\u001b[1;32m--> 998\u001b[0m         save(v)\n\u001b[0;32m    999\u001b[0m     write(SETITEMS)\n\u001b[0;32m   1000\u001b[0m \u001b[39melif\u001b[39;00m n:\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\u001b[39m\"\u001b[39m\u001b[39mTuple returned by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m must have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mtwo to six elements\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m reduce)\n\u001b[0;32m    602\u001b[0m \u001b[39m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_reduce(obj\u001b[39m=\u001b[39;49mobj, \u001b[39m*\u001b[39;49mrv)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[1;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     save(func)\n\u001b[1;32m--> 692\u001b[0m     save(args)\n\u001b[0;32m    693\u001b[0m     write(REDUCE)\n\u001b[0;32m    695\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     \u001b[39m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[39m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[39m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    886\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m obj:\n\u001b[1;32m--> 887\u001b[0m         save(element)\n\u001b[0;32m    888\u001b[0m     \u001b[39m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mid\u001b[39m(obj) \u001b[39min\u001b[39;00m memo:\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:284\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    281\u001b[0m     wrapper\u001b[39m.\u001b[39mwrite_array(obj, \u001b[39mself\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49msave(\u001b[39mself\u001b[39;49m, obj)\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[1;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[0;32m    558\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mget(t)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     f(\u001b[39mself\u001b[39;49m, obj)  \u001b[39m# Call unbound method with explicit self\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Duraisethupathy\\anaconda3\\lib\\pickle.py:1071\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[1;34m(self, obj, name)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     obj2, parent \u001b[39m=\u001b[39m _getattribute(module, name)\n\u001b[0;32m   1070\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mImportError\u001b[39;00m, \u001b[39mKeyError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m):\n\u001b[1;32m-> 1071\u001b[0m     \u001b[39mraise\u001b[39;00m PicklingError(\n\u001b[0;32m   1072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt pickle \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m: it\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms not found as \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1073\u001b[0m         (obj, module_name, name)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1074\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m     \u001b[39mif\u001b[39;00m obj2 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m obj:\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function IBMModel.reset_probabilities.<locals>.<lambda> at 0x0000023AE5A20940>: it's not found as nltk.translate.ibm_model.IBMModel.reset_probabilities.<locals>.<lambda>"
     ]
    }
   ],
   "source": [
    "joblib.dump(de_eng_m, 'D2E-Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unigram-Counts']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(unigram_counts, 'Unigram-Counts')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b47566efb25eea7c5a36f8d4e14e44cbbde08c492e229843ca267b4ef9a6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
